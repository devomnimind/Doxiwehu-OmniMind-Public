# OmniMind - Environment Variables Template
# Copy this to .env and fill with your actual values
# NEVER commit .env to version control

# ================================
# Neural Backends
# ================================

# HuggingFace Configuration
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
# Get your token from: https://huggingface.co/settings/tokens
# Required for: Downloading models from HuggingFace Hub

MODEL_ID=Qwen/Qwen2.5-0.5B-Instruct
# Default model for neural component
# Alternatives: meta-llama/Llama-2-7b-hf, gpt2, etc.

HF_SPACE_URL=
# Optional: HuggingFace Space URL for inference API
# Example: https://username-space-name.hf.space

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
# Local Ollama server endpoint
# Install Ollama: https://ollama.ai

# ================================
# Vector Database (Qdrant)
# ================================

# Qdrant Local Configuration
QDRANT_URL=http://localhost:6333
# Local Qdrant server endpoint
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Qdrant Cloud Configuration (Optional)
OMNIMIND_QDRANT_CLOUD_URL=
# Example: https://xyz.qdrant.cloud:6333
OMNIMIND_QDRANT_API_KEY=
# Your Qdrant Cloud API key
OMNIMIND_QDRANT_COLLECTION=omnimind_memory
# Collection name for vector storage
OMNIMIND_QDRANT_VECTOR_SIZE=768
# Vector dimension (must match embedding model)

# ================================
# Redis (Cache & Pub/Sub)
# ================================

REDIS_HOST=localhost
REDIS_PORT=6379
# Redis server for caching and message queuing

# ================================
# Supabase (Optional Cloud Backend)
# ================================

OMNIMIND_SUPABASE_URL=
# Example: https://xxxxx.supabase.co
OMNIMIND_SUPABASE_ANON_KEY=
# Supabase anonymous key
OMNIMIND_SUPABASE_SERVICE_ROLE_KEY=
# Supabase service role key (keep secret!)
OMNIMIND_SUPABASE_PROJECT=
# Your Supabase project reference

# ================================
# API Keys (Optional)
# ================================

OPENAI_API_KEY=
# Optional: OpenAI API key for fallback
# Get from: https://platform.openai.com/api-keys

# ================================
# Application Configuration
# ================================

DEBUG=false
# Set to true for development mode
# Production: false

LOG_LEVEL=INFO
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL

ENVIRONMENT=development
# Environment: development | staging | production

OMNIMIND_WORKSPACE=.
# Workspace directory for OmniMind daemon
# Default: current directory

OMNIMIND_CLOUD_ENABLED=true
# Enable cloud features (Supabase, Qdrant Cloud)
# Set to false for fully local operation

# ================================
# Hardware & GPU
# ================================

# CUDA Configuration (Uncomment if needed)
# CUDA_VISIBLE_DEVICES=0
# Force specific GPU (0, 1, etc.)
# Leave commented to use all available GPUs

# ================================
# Security & Firecracker
# ================================

OMNIMIND_FIRECRACKER_KERNEL=
# Path to Firecracker kernel image
# Example: /opt/firecracker/vmlinux.bin

OMNIMIND_FIRECRACKER_ROOTFS=
# Path to Firecracker rootfs image
# Example: /opt/firecracker/rootfs.ext4

OMNIMIND_DLP_POLICY_FILE=
# Path to DLP (Data Loss Prevention) policy file
# Example: /etc/omnimind/dlp_policy.json

# ================================
# System & Metadata
# ================================

USER=omnimind
# Username for audit logging
# Default: system username

OMNIMIND_TIMESTAMP=
# Optional: Override timestamp for testing
# Format: ISO 8601 (2025-11-24T10:00:00Z)

CURRENT_TIME=
# Optional: Override current time for HSM operations
# Format: ISO 8601 (2025-11-24T10:00:00Z)

BUILD_TIMESTAMP=
# Optional: Build timestamp for config validation
# Set automatically by CI/CD

# ================================
# Development & Testing
# ================================

# Pytest Configuration
# PYTEST_CURRENT_TEST=
# Set automatically by pytest

# Coverage Configuration
# COV_CORE_SOURCE=src
# Set automatically by pytest-cov

# ================================
# Notes
# ================================

# Minimal Configuration (Local Only):
# - No API keys needed
# - Use local Ollama for neural backend
# - Use local Qdrant (Docker: docker run -p 6333:6333 qdrant/qdrant)
# - Use local Redis (Docker: docker run -p 6379:6379 redis)

# Recommended for Development:
# - HUGGING_FACE_HUB_TOKEN (for model downloads)
# - OLLAMA_HOST=http://localhost:11434
# - QDRANT_URL=http://localhost:6333
# - REDIS_HOST=localhost
# - DEBUG=true
# - LOG_LEVEL=DEBUG

# Production Checklist:
# - Set DEBUG=false
# - Set ENVIRONMENT=production
# - Set LOG_LEVEL=INFO or WARNING
# - Configure Firecracker paths (if using sandboxing)
# - Set DLP_POLICY_FILE (if using data loss prevention)
# - Review all secrets and API keys
# - Never commit .env to git!
