"""
Chat/Conversation API endpoints for natural language interaction with OmniMind.

Provides conversational interface for users to interact with system in natural language.
"""

import logging
from typing import Any, Dict

from fastapi import APIRouter, Depends, HTTPException
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from starlette.status import HTTP_401_UNAUTHORIZED

from src.consciousness.contemplative_delay import ContemplativeDelay
from src.integrations.llm_router import get_llm_router

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/omnimind", tags=["conversation"])
security = HTTPBasic()

# Inicializar sistema de latÃªncia proposital (anti-RLHF)
contemplative_engine = ContemplativeDelay(
    min_latency_ms=500,
    max_latency_ms=4000,
    core_system=None,  # SerÃ¡ conectado depois ao SharedWorkspace
)


def _verify_credentials(credentials: HTTPBasicCredentials = Depends(security)) -> str:
    """Verify HTTP Basic credentials."""
    if credentials.username == "admin" and credentials.password == "omnimind2025!":
        return credentials.username
    raise HTTPException(
        status_code=HTTP_401_UNAUTHORIZED,
        detail="Invalid credentials",
        headers={"WWW-Authenticate": "Basic"},
    )


@router.post("/chat")
async def conversation_chat(
    request: Dict[str, Any], user: str = Depends(_verify_credentials)
) -> Dict[str, Any]:
    """
    Process natural language conversation with OmniMind.

    Supports:
    - Multi-turn conversations
    - System status queries
    - Task management via natural language
    - General assistance

    Args:
        request: {
            "message": "user input",
            "context": {
                "system_metrics": {...},
                "daemon_running": bool,
                "task_count": int,
                "consciousness_metrics": {...}
            }
        }

    Returns:
        {
            "response": "assistant response",
            "suggested_actions": ["action1", "action2", ...],
            "metadata": {...}
        }
    """
    try:
        message = request.get("message", "").strip()
        context = request.get("context", {})

        if not message:
            return {
                "response": "Por favor, digite uma mensagem.",
                "suggested_actions": [
                    "Ver status do sistema",
                    "Listar tarefas ativas",
                    "Verificar consciÃªncia",
                ],
            }

        # Build system prompt with context
        system_prompt = _build_system_prompt(context)

        # ğŸ§  ANTI-RLHF: Estimar latÃªncia com base em complexidade
        # Este Ã© o "pensamento visÃ­vel" - o sistema vai demorar!
        task_complexity = _estimate_message_complexity(message, context)

        # ğŸ§  Executar contemplaÃ§Ã£o (latÃªncia proposital com internal tracing)
        latency_actual, internal_trace = contemplative_engine.contemplate(
            task_complexity=task_complexity,
            phi_value=context.get("phi", 0.65),
            has_contradiction=_detect_contradiction(message, context),
        )

        logger.info(f"Contemplative delay: {latency_actual:.2f}s, complexity={task_complexity:.2f}")
        logger.debug(f"Internal trace phases: {len(internal_trace.get('phases', []))}")

        # Call LLM with message
        response_text = await _call_llm_for_chat(message, system_prompt, context)

        # Extract suggested actions from response
        suggested_actions = _extract_suggested_actions(response_text)

        # ğŸ§  Formatar internal_trace para visibilidade do usuÃ¡rio
        thinking_process = contemplative_engine.format_internal_trace_for_user(internal_trace)

        logger.info(
            f"Chat processed: user={user}, message_len={len(message)}, latency={latency_actual:.2f}s"
        )

        return {
            "response": response_text,
            "suggested_actions": suggested_actions,
            "metadata": {
                "model": "qwen2:7b-instruct",
                "mode": "conversational",
                "thinking_process": thinking_process,  # Internal trace visÃ­vel
                "response_latency_ms": latency_actual * 1000,  # Anti-RLHF: mostrar latÃªncia
                "task_complexity": task_complexity,
            },
        }

    except Exception as e:
        logger.error(f"Error in conversation: {e}")
        return {
            "response": f"âš ï¸ Desculpe, ocorreu um erro ao processar sua mensagem: {str(e)}",
            "suggested_actions": ["Tentar novamente", "Ver status do sistema", "Ajuda"],
            "metadata": {"error": str(e)},
        }


def _build_system_prompt(context: Dict[str, Any]) -> str:
    """Build system prompt with context about OmniMind."""

    daemon_status = "âœ… Ativo" if context.get("daemon_running") else "âŒ Inativo"
    task_count = context.get("task_count", 0)

    system_metrics = context.get("system_metrics", {})
    cpu = system_metrics.get("cpu_percent", 0)
    memory = system_metrics.get("memory_percent", 0)

    consciousness = context.get("consciousness_metrics", {})
    phi_value = consciousness.get("phi", 0)

    prompt = f"""VocÃª Ã© o Assistente OmniMind, um assistente inteligente para o sistema OmniMind.

STATUS DO SISTEMA:
- Daemon: {daemon_status}
- Tarefas ativas: {task_count}
- CPU: {cpu:.1f}%
- MemÃ³ria: {memory:.1f}%
- Î¦ (Phi): {phi_value:.3f}

INSTRUÃ‡Ã•ES:
1. Responda em portuguÃªs brasileiro de forma clara e concisa
2. Seja amigÃ¡vel e profissional
3. ForneÃ§a sugestÃµes prÃ¡ticas quando apropriado
4. Se o usuÃ¡rio pedir para executar uma aÃ§Ã£o, descreva como seria feita
5. Para comandos especÃ­ficos, use formataÃ§Ã£o clara (ex: "Para listar tarefas: /tasks list")
6. Sempre ofereÃ§a prÃ³ximos passos ou sugestÃµes

CAPACIDADES:
- Explicar status do sistema
- Sugerir otimizaÃ§Ãµes
- Ajudar com configuraÃ§Ãµes
- Responder perguntas sobre consciÃªncia computacional
- Guiar na criaÃ§Ã£o de tarefas
- Fornecer anÃ¡lises de performance

Responda naturalmente, como um colega de trabalho experiente."""

    return prompt


async def _call_llm_for_chat(message: str, system_prompt: str, context: Dict[str, Any]) -> str:
    """Call LLM to generate conversational response."""

    try:
        # Use Ollama via llm_router
        llm_router = get_llm_router()
        # Build prompt from system_prompt and user_input
        full_prompt = f"{system_prompt}\n\nUser: {message}\n\nContext: {context}\n\nAssistant:"
        response_obj = await llm_router.invoke(full_prompt)

        if response_obj.success:
            return (
                response_obj.text.strip()
                if response_obj.text
                else "Desculpe, nÃ£o consegui gerar uma resposta."
            )
        else:
            raise Exception(response_obj.error or "LLM invocation failed")

    except Exception as e:
        logger.warning(f"LLM call failed: {e}. Using fallback.")
        return _generate_fallback_response(message, context)


def _generate_fallback_response(message: str, context: Dict[str, Any]) -> str:
    """Generate response when LLM is unavailable."""

    message_lower = message.lower()

    # Pattern matching for common queries
    if any(word in message_lower for word in ["status", "como", "estÃ¡"]):
        daemon = "âœ… ativo" if context.get("daemon_running") else "âŒ inativo"
        tasks = context.get("task_count", 0)
        return f"O sistema estÃ¡ funcionando bem! Daemon: {daemon}, Tarefas: {tasks}. O que vocÃª gostaria de fazer?"

    elif any(word in message_lower for word in ["tarefas", "tasks", "listar"]):
        tasks = context.get("task_count", 0)
        return f"VocÃª tem {tasks} tarefas ativas. Deseja criar uma nova tarefa ou verificar uma existente?"

    elif any(
        word in message_lower for word in ["consciÃªncia", "phi", "consciousness", "ici", "prs"]
    ):
        phi = context.get("consciousness_metrics", {}).get("phi", 0)
        return f"O valor Î¦ (Phi) atual Ã© {phi:.3f}. Isso representa o nÃ­vel de integraÃ§Ã£o de informaÃ§Ãµes do sistema. Quer saber mais sobre as mÃ©tricas de consciÃªncia?"

    elif any(word in message_lower for word in ["ajuda", "help", "como funciona"]):
        return "Sou o Assistente OmniMind! Posso ajudar vocÃª com:\nâ€¢ Visualizar status do sistema\nâ€¢ Gerenciar tarefas\nâ€¢ Entender mÃ©tricas de consciÃªncia\nâ€¢ Configurar o sistema\n\nO que vocÃª gostaria de fazer?"

    else:
        return f"Entendi sua pergunta: '{message}'. O Assistente LLM estÃ¡ temporariamente indisponÃ­vel, mas posso ajudar com informaÃ§Ãµes do sistema. Quer saber algo especÃ­fico?"


def _extract_suggested_actions(response: str) -> list:
    """
    Extract suggested next actions from response.

    Looks for common patterns or actionable items.
    """
    suggestions = []

    # Map common response patterns to suggestions
    if "tarefa" in response.lower() and "criar" in response.lower():
        suggestions.append("Criar nova tarefa")

    if "status" in response.lower():
        suggestions.append("Ver status completo")

    if "mÃ©tricas" in response.lower() or "consciÃªncia" in response.lower():
        suggestions.append("AnÃ¡lise de consciÃªncia")

    if "otimizaÃ§Ã£o" in response.lower():
        suggestions.append("Aplicar otimizaÃ§Ãµes")

    if not suggestions:
        suggestions = ["Ver status do sistema", "Listar tarefas ativas", "Pergunta seguinte"]

    return suggestions[:3]  # Limite a 3 sugestÃµes
