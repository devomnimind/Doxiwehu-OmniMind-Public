#!/usr/bin/env python3
"""
OmniMind Complete System Indexer

Indexa TODA a m√°quina: arquivos, documentos, configura√ß√µes, software, etc.
Inclui HD externo e detecta automaticamente tipos de conte√∫do.

Funcionalidades:
- Indexa√ß√£o completa de disco (como chkdsk /f no Windows)
- Detec√ß√£o autom√°tica de tipos de arquivo
- Suporte a HD externo
- Estat√≠sticas detalhadas
- Busca sem√¢ntica universal
"""

import os
import sys
import logging
import hashlib
import mimetypes
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed

import psutil
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models as qmodels

# For√ßar CPU para evitar problemas de mem√≥ria
os.environ["CUDA_VISIBLE_DEVICES"] = ""

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

logger = logging.getLogger(__name__)


class UniversalContentType(Enum):
    """Tipos de conte√∫do universais para indexa√ß√£o completa."""

    CODE = "code"
    DOCUMENTATION = "documentation"
    CONFIG = "config"
    DATA = "data"
    BINARY = "binary"
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    ARCHIVE = "archive"
    SYSTEM = "system"
    UNKNOWN = "unknown"


@dataclass
class UniversalContentChunk:
    """Chunk universal de qualquer tipo de conte√∫do."""

    file_path: str
    content: str
    content_type: UniversalContentType
    mime_type: str
    size_bytes: int
    is_text: bool
    encoding: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class UniversalEmbeddingsIndexer:
    """
    Indexador universal que pode processar QUALQUER arquivo na m√°quina.

    Similar ao "chkdsk /f" do Windows, mas para embeddings sem√¢nticos.
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "universal_machine_embeddings",
        model_name: str = "all-MiniLM-L6-v2",
        max_file_size_mb: int = 10,  # M√°ximo 10MB por arquivo
        chunk_size: int = 1000,  # Caracteres por chunk
        max_workers: int = 4,  # Processamento paralelo
    ):
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.model_name = model_name
        self.max_file_size_mb = max_file_size_mb
        self.chunk_size = chunk_size
        self.max_workers = max_workers

        # Inicializar modelo
        logger.info(f"Carregando modelo universal: {model_name}")
        self.model = SentenceTransformer(model_name, device="cpu")
        self.embedding_dim = self.model.get_sentence_embedding_dimension()

        # Inicializar Qdrant
        self.client = QdrantClient(qdrant_url)
        self._ensure_collection()

        # Estat√≠sticas
        self.stats = {
            "files_processed": 0,
            "files_indexed": 0,
            "chunks_created": 0,
            "bytes_processed": 0,
            "errors": 0,
            "by_type": {},
            "by_extension": {},
        }

        # Cache de tipos MIME
        mimetypes.init()

        logger.info("ü§ñ Universal Embeddings Indexer inicializado")
        logger.info(f"üìä Modelo: {model_name} (dim={self.embedding_dim})")
        logger.info(f"üéØ M√°ximo por arquivo: {max_file_size_mb}MB")
        logger.info(f"‚ö° Workers paralelos: {max_workers}")

    def _ensure_collection(self):
        """Cria cole√ß√£o universal se n√£o existir."""
        try:
            collections = self.client.get_collections().collections or []
            collection_names = [info.name for info in collections]

            if self.collection_name not in collection_names:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=qmodels.VectorParams(
                        size=self.embedding_dim, distance=qmodels.Distance.COSINE
                    ),
                )
                logger.info(f"üìÅ Cole√ß√£o universal criada: {self.collection_name}")
        except Exception as exc:
            logger.error(f"‚ùå Erro ao criar cole√ß√£o: {exc}")
            raise

    def detect_content_type(self, file_path: str) -> UniversalContentType:
        """Detecta tipo de conte√∫do baseado em MIME type e extens√£o."""
        path = Path(file_path)
        ext = path.suffix.lower()

        # Detectar por MIME type
        mime_type, _ = mimetypes.guess_type(str(path))

        if mime_type:
            if mime_type.startswith("text/"):
                # Arquivos de c√≥digo
                code_exts = {".py", ".js", ".ts", ".java", ".cpp", ".c", ".go", ".rs", ".php", ".rb", ".sh", ".sql"}
                if ext in code_exts:
                    return UniversalContentType.CODE

                # Documenta√ß√£o
                doc_exts = {".md", ".txt", ".rst", ".adoc", ".pdf"}
                if ext in doc_exts:
                    return UniversalContentType.DOCUMENTATION

                # Configura√ß√µes
                config_exts = {".yaml", ".yml", ".json", ".toml", ".ini", ".cfg", ".conf"}
                if ext in config_exts:
                    return UniversalContentType.CONFIG

                return UniversalContentType.TEXT

            elif mime_type.startswith("image/"):
                return UniversalContentType.IMAGE
            elif mime_type.startswith("audio/"):
                return UniversalContentType.AUDIO
            elif mime_type.startswith("video/"):
                return UniversalContentType.VIDEO
            elif mime_type in ["application/zip", "application/x-tar", "application/gzip"]:
                return UniversalContentType.ARCHIVE
            elif mime_type.startswith("application/"):
                return UniversalContentType.BINARY

        # Detectar por extens√£o (fallback)
        if ext in [".db", ".sqlite", ".csv", ".xlsx", ".xls"]:
            return UniversalContentType.DATA
        elif ext in [".exe", ".dll", ".so", ".dylib"]:
            return UniversalContentType.BINARY
        elif ext in [".log", ".out", ".err"]:
            return UniversalContentType.SYSTEM

        return UniversalContentType.UNKNOWN

    def can_process_file(self, file_path: str) -> bool:
        """Verifica se arquivo pode ser processado."""
        try:
            path = Path(file_path)

            # Verificar tamanho
            size_mb = path.stat().st_size / (1024 * 1024)
            if size_mb > self.max_file_size_mb:
                return False

            # Verificar se √© arquivo regular
            if not path.is_file():
                return False

            # Verificar permiss√µes
            if not os.access(path, os.R_OK):
                return False

            return True

        except Exception:
            return False

    def extract_text_content(self, file_path: str) -> Optional[str]:
        """Extrai conte√∫do textual de qualquer arquivo."""
        try:
            path = Path(file_path)
            content_type = self.detect_content_type(file_path)

            # Arquivos de texto direto
            if content_type in [UniversalContentType.CODE, UniversalContentType.TEXT,
                              UniversalContentType.CONFIG, UniversalContentType.DOCUMENTATION]:

                # Tentar diferentes encodings
                encodings = ["utf-8", "latin-1", "cp1252", "iso-8859-1"]
                for encoding in encodings:
                    try:
                        with open(path, "r", encoding=encoding) as f:
                            content = f.read()
                            # Limitar tamanho para evitar problemas de mem√≥ria
                            if len(content) > 100000:  # 100KB max
                                content = content[:100000] + "...[TRUNCATED]"
                            return content
                    except UnicodeDecodeError:
                        continue

            # Arquivos PDF (se pdftotext estiver dispon√≠vel)
            elif path.suffix.lower() == ".pdf":
                try:
                    result = subprocess.run(
                        ["pdftotext", "-layout", "-enc", "UTF-8", str(path), "-"],
                        capture_output=True, text=True, timeout=30
                    )
                    if result.returncode == 0:
                        return result.stdout
                except (subprocess.TimeoutExpired, FileNotFoundError):
                    pass

            # Arquivos bin√°rios - extrair metadados
            else:
                # Para arquivos bin√°rios, criar descri√ß√£o baseada em metadados
                stat = path.stat()
                mime_type, _ = mimetypes.guess_type(str(path))

                metadata = f"""
Arquivo: {path.name}
Tamanho: {stat.st_size} bytes
Tipo MIME: {mime_type or 'desconhecido'}
Modificado: {stat.st_mtime}
Permiss√µes: {oct(stat.st_mode)}
Localiza√ß√£o: {path.parent}
"""

                return metadata.strip()

        except Exception as e:
            logger.debug(f"Erro ao extrair conte√∫do de {file_path}: {e}")

        return None

    def chunk_content(self, file_path: str) -> List[UniversalContentChunk]:
        """Divide arquivo em chunks process√°veis."""
        content = self.extract_text_content(file_path)
        if not content:
            return []

        content_type = self.detect_content_type(file_path)
        path = Path(file_path)
        mime_type, _ = mimetypes.guess_type(str(path))

        # Para arquivos pequenos, um chunk s√≥
        if len(content) <= self.chunk_size:
            return [UniversalContentChunk(
                file_path=file_path,
                content=content,
                content_type=content_type,
                mime_type=mime_type or "unknown",
                size_bytes=path.stat().st_size,
                is_text=True,
                metadata={"chunk_index": 0, "total_chunks": 1}
            )]

        # Dividir em chunks com sobreposi√ß√£o
        chunks = []
        overlap = min(200, self.chunk_size // 4)  # 25% de sobreposi√ß√£o

        i = 0
        chunk_index = 0
        total_chunks = (len(content) + self.chunk_size - overlap - 1) // (self.chunk_size - overlap)

        while i < len(content):
            end = min(i + self.chunk_size, len(content))
            chunk_content = content[i:end]

            chunks.append(UniversalContentChunk(
                file_path=file_path,
                content=chunk_content,
                content_type=content_type,
                mime_type=mime_type or "unknown",
                size_bytes=path.stat().st_size,
                is_text=True,
                metadata={
                    "chunk_index": chunk_index,
                    "total_chunks": total_chunks,
                    "start_pos": i,
                    "end_pos": end
                }
            ))

            i += self.chunk_size - overlap
            chunk_index += 1

        return chunks

    def index_file(self, file_path: str) -> int:
        """Indexa um arquivo individual."""
        self.stats["files_processed"] += 1

        try:
            if not self.can_process_file(file_path):
                return 0

            # Criar chunks
            chunks = self.chunk_content(file_path)
            if not chunks:
                return 0

            # Gerar embeddings e armazenar
            points = []
            for chunk in chunks:
                try:
                    # Gerar embedding
                    embedding = self.model.encode(chunk.content, normalize_embeddings=True)

                    # Criar ID √∫nico
                    content_hash = hashlib.sha256(
                        f"{chunk.file_path}:{chunk.content}".encode()
                    ).hexdigest()[:16]
                    point_id = int(content_hash, 16)

                    # Payload com metadados
                    payload = {
                        "file_path": chunk.file_path,
                        "content": chunk.content[:2000],  # Limitar tamanho
                        "content_type": chunk.content_type.value,
                        "mime_type": chunk.mime_type,
                        "size_bytes": chunk.size_bytes,
                        "is_text": chunk.is_text,
                        "chunk_metadata": chunk.metadata or {},
                    }

                    points.append(
                        qmodels.PointStruct(id=point_id, vector=embedding.tolist(), payload=payload)
                    )

                except Exception as e:
                    logger.debug(f"Erro ao processar chunk de {file_path}: {e}")
                    continue

            # Upsert no Qdrant
            if points:
                self.client.upsert(collection_name=self.collection_name, points=points)

                # Atualizar estat√≠sticas
                self.stats["files_indexed"] += 1
                self.stats["chunks_created"] += len(points)
                self.stats["bytes_processed"] += chunks[0].size_bytes

                # Estat√≠sticas por tipo
                ct = chunks[0].content_type.value
                self.stats["by_type"][ct] = self.stats["by_type"].get(ct, 0) + 1

                # Estat√≠sticas por extens√£o
                ext = Path(file_path).suffix.lower()
                self.stats["by_extension"][ext] = self.stats["by_extension"].get(ext, 0) + 1

                logger.debug(f"‚úÖ Indexado: {file_path} ({len(points)} chunks)")
                return len(points)

        except Exception as e:
            self.stats["errors"] += 1
            logger.debug(f"‚ùå Erro ao indexar {file_path}: {e}")

        return 0

    def get_mount_points(self) -> List[str]:
        """Detecta todos os pontos de montagem (incluindo HD externo)."""
        mount_points = []

        try:
            # Usar psutil para detectar parti√ß√µes
            partitions = psutil.disk_partitions(all=True)

            for partition in partitions:
                mount_point = partition.mountpoint

                # Filtrar pontos de montagem relevantes
                if (os.path.exists(mount_point) and
                    os.access(mount_point, os.R_OK) and
                    not any(skip in mount_point for skip in ["/proc", "/sys", "/dev", "/run"])):
                    mount_points.append(mount_point)

        except Exception as e:
            logger.warning(f"Erro ao detectar pontos de montagem: {e}")
            # Fallback: pontos comuns
            mount_points = ["/", "/home", "/mnt", "/media"]

        return sorted(set(mount_points))

    def index_entire_machine(self, exclude_patterns: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Indexa TODA a m√°quina - como "chkdsk /f" mas para embeddings.

        Args:
            exclude_patterns: Padr√µes de caminho a excluir (regex)
        """
        if exclude_patterns is None:
            exclude_patterns = [
                r"/proc/.*",
                r"/sys/.*",
                r"/dev/.*",
                r"/run/.*",
                r"/tmp/.*",
                r"/var/tmp/.*",
                r".*/\.git/.*",
                r".*/node_modules/.*",
                r".*/__pycache__/.*",
                r".*/\.cache/.*",
                r".*\.pyc$",
                r".*\.pyo$",
            ]

        logger.info("üöÄ Iniciando indexa√ß√£o COMPLETA da m√°quina")
        logger.info("üí° Isso pode levar HORAS dependendo do tamanho dos discos")

        # Detectar pontos de montagem
        mount_points = self.get_mount_points()
        logger.info(f"üìç Pontos de montagem detectados: {mount_points}")

        total_files_found = 0
        total_chunks_created = 0

        # Processar cada ponto de montagem
        for mount_point in mount_points:
            logger.info(f"üîç Indexando: {mount_point}")
            mount_chunks = self._index_mount_point(mount_point, exclude_patterns)
            total_chunks_created += mount_chunks

        # Estat√≠sticas finais
        final_stats = self.get_stats()
        logger.info("üéâ Indexa√ß√£o completa da m√°quina finalizada!")
        logger.info(f"üìä Total processado: {final_stats['files_processed']} arquivos")
        logger.info(f"‚úÖ Total indexado: {final_stats['files_indexed']} arquivos")
        logger.info(f"üß© Total chunks: {final_stats['chunks_created']}")
        logger.info(f"üíæ Total bytes: {final_stats['bytes_processed'] / (1024**3):.2f} GB")

        return final_stats

    def _index_mount_point(self, mount_point: str, exclude_patterns: List[str]) -> int:
        """Indexa um ponto de montagem espec√≠fico."""
        chunks_created = 0

        try:
            # Coletar todos os arquivos
            all_files = []
            for root, dirs, files in os.walk(mount_point):
                # Aplicar exclus√µes
                for pattern in exclude_patterns:
                    import re
                    if re.search(pattern, root):
                        dirs[:] = []  # N√£o entrar neste diret√≥rio
                        break

                for file in files:
                    file_path = os.path.join(root, file)
                    all_files.append(file_path)

            logger.info(f"üìÇ Encontrados {len(all_files)} arquivos em {mount_point}")

            # Processar em paralelo
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self.index_file, file_path) for file_path in all_files]

                for future in as_completed(futures):
                    try:
                        chunks = future.result()
                        chunks_created += chunks
                    except Exception as e:
                        logger.debug(f"Erro em future: {e}")

        except Exception as e:
            logger.error(f"Erro ao indexar {mount_point}: {e}")

        return chunks_created

    def search_universal(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """Busca sem√¢ntica universal em todo o conte√∫do indexado."""
        # Gerar embedding da query
        query_embedding = self.model.encode(query, normalize_embeddings=True)

        # Buscar no Qdrant
        search_result = self.client.query_points(
            collection_name=self.collection_name,
            query=query_embedding.tolist(),
            limit=top_k,
            with_payload=True,
            with_vectors=False,
        )

        # Formatar resultados
        results = []
        for point in search_result.points:
            payload = point.payload or {}
            results.append({
                "score": float(point.score),
                "file_path": payload.get("file_path", ""),
                "content": payload.get("content", ""),
                "content_type": payload.get("content_type", ""),
                "mime_type": payload.get("mime_type", ""),
                "size_bytes": payload.get("size_bytes", 0),
                "is_text": payload.get("is_text", False),
                "chunk_metadata": payload.get("chunk_metadata", {}),
            })

        return results

    def get_stats(self) -> Dict[str, Any]:
        """Estat√≠sticas detalhadas da indexa√ß√£o."""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            base_stats = {
                "collection_name": self.collection_name,
                "vector_dim": self.embedding_dim,
                "total_chunks": collection_info.points_count,
                "model": self.model_name,
            }
        except Exception:
            base_stats = {"error": "N√£o foi poss√≠vel obter stats da cole√ß√£o"}

        # Combinar com stats locais
        base_stats.update(self.stats)
        return base_stats


def main():
    """Fun√ß√£o principal para indexa√ß√£o completa."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )

    logger.info("ü§ñ OMNIMIND - Indexa√ß√£o Universal da M√°quina")
    logger.info("=" * 60)

    # Verificar depend√™ncias
    try:
        import sentence_transformers
        import qdrant_client
        import psutil
        logger.info("‚úÖ Depend√™ncias OK")
    except ImportError as e:
        logger.error(f"‚ùå Depend√™ncia faltando: {e}")
        sys.exit(1)

    # Verificar Qdrant
    try:
        client = QdrantClient("http://localhost:6333")
        collections = client.get_collections()
        logger.info("‚úÖ Qdrant OK")
    except Exception as e:
        logger.error(f"‚ùå Qdrant inacess√≠vel: {e}")
        logger.error("üí° Execute: docker run -p 6333:6333 qdrant/qdrant")
        sys.exit(1)

    # Inicializar indexador universal
    indexer = UniversalEmbeddingsIndexer()

    # Indexar m√°quina completa
    try:
        logger.info("üöÄ Iniciando indexa√ß√£o COMPLETA...")
        logger.info("‚ö†Ô∏è  Isso pode levar muito tempo!")

        stats = indexer.index_entire_machine()

        logger.info("\nüéâ Indexa√ß√£o conclu√≠da!")
        logger.info("üìä Estat√≠sticas finais:")
        for key, value in stats.items():
            if isinstance(value, dict):
                logger.info(f"   {key}:")
                for subkey, subvalue in value.items():
                    logger.info(f"      {subkey}: {subvalue}")
            else:
                logger.info(f"   {key}: {value}")

    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è  Indexa√ß√£o interrompida pelo usu√°rio")
        stats = indexer.get_stats()
        logger.info("üìä Estat√≠sticas parciais salvas")

    except Exception as e:
        logger.error(f"‚ùå Erro durante indexa√ß√£o: {e}")
        sys.exit(1)

    # Teste de busca
    try:
        logger.info("\nüîç Testando busca universal...")
        test_queries = [
            "sistema de arquivos Linux",
            "configura√ß√£o de rede",
            "c√≥digo Python para machine learning",
            "documenta√ß√£o de API",
        ]

        for query in test_queries:
            logger.info(f"\nüîé '{query}':")
            results = indexer.search_universal(query, top_k=3)

            for i, result in enumerate(results, 1):
                logger.info(f"   {i}. [{result['content_type']}] {result['file_path']}")
                logger.info(f"      Score: {result['score']:.3f}")
                logger.info(f"      Conte√∫do: {result['content'][:100]}...")

    except Exception as e:
        logger.error(f"‚ùå Erro no teste de busca: {e}")

    logger.info("\nüéØ Sistema pronto para buscas sem√¢nticas universais!")
    logger.info("\nüí° Uso:")
    logger.info("   from universal_indexer import UniversalEmbeddingsIndexer")
    logger.info("   indexer = UniversalEmbeddingsIndexer()")
    logger.info("   results = indexer.search_universal('sua consulta')")


if __name__ == "__main__":
    main()