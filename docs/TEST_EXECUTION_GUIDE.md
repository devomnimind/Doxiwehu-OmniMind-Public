# ğŸ“– GUIA COMPLETO DE EXECUÃ‡ÃƒO DE TESTES - OmniMind

**Data**: 29 de Novembro de 2025  
**Status**: âœ… VALIDADO NA MÃQUINA GTX 1650  
**Autor**: Classificador automÃ¡tico de testes + DocumentaÃ§Ã£o honesta

---

## ğŸ¯ OBJETIVO DESTE GUIA

VocÃª tem:
- âœ… 798 testes MOCK (estrutura verificada)
- âœ… 3031 testes SEMI-REAL (GPU funciona)
- âŒ 0 testes REAL (PRECISA criar)

Este guia mostra como executar cada tipo e entender o que eles significam para seu paper.

---

## âš¡ INÃCIO RÃPIDO

### Se vocÃª tem 2 MINUTOS (validaÃ§Ã£o rÃ¡pida)
```bash
cd /home/fahbrain/projects/omnimind

# Rodar testes MOCK (estrutura)
bash scripts/run_tests_by_category.sh 1
```

**Resultado**: Valida que cÃ³digo estÃ¡ bem estruturado, nenhum crash Ã³bvio.

### Se vocÃª tem 12 MINUTOS (validaÃ§Ã£o mÃ©dia)
```bash
# Rodar MOCK + SEMI-REAL
bash scripts/run_tests_by_category.sh 3
```

**Resultado**: Valida estrutura + GPU funciona (PyTorch real, LLM mockado).

### Se vocÃª tem 30+ MINUTOS (validaÃ§Ã£o REAL para paper)
```bash
# Rodar testes REAL - mede Î¦ de verdade
bash scripts/run_tests_by_category.sh 4
```

**Resultado**: Valor REAL de Î¦ medido, tempo documentado, pronto para paper.

### Se vocÃª tem 2 HORAS (validaÃ§Ã£o completa)
```bash
# Rodar TUDO
bash scripts/run_tests_by_category.sh 5
```

**Resultado**: Suite completa, cobertura 100%, documentaÃ§Ã£o total.

---

## ğŸ“š EXPLICAÃ‡ÃƒO DOS TIPOS DE TESTE

### ğŸ”µ [MOCK] - Testes com @patch (798 testes, 2 minutos)

**O que sÃ£o**:
```python
@patch("src.agents.orchestrator_agent.OmniMindCore")
def test_delegate_task(mock_core):
    # NÃ£o toca OmniMindCore REAL
    # Valida sÃ³ a lÃ³gica de delegaÃ§Ã£o
    pass
```

**O que testam**:
- âœ… Estrutura de cÃ³digo (classes, mÃ©todos, interfaces)
- âœ… LÃ³gica de negÃ³cio (fluxo de orquestraÃ§Ã£o)
- âœ… Error handling (quando as coisas dÃ£o errado)

**O que NÃƒO testam**:
- âŒ GPU real (PyTorch)
- âŒ LLM real (Ollama, OpenRouter)
- âŒ MÃ©tricas reais (Î¦, ablaÃ§Ã£o)
- âŒ Performance real

**Quando usar**:
- Desenvolvimento rÃ¡pido (feedback em 2 min)
- MudanÃ§as em agentes/orquestraÃ§Ã£o
- PR validaÃ§Ã£o antes de push

**Para seu paper**:
âœ… Posso afirmar: "Arquitetura estruturalmente sÃ³lida"  
âŒ NÃ£o posso afirmar: "Î¦ = 0.8667"

---

### ğŸŸ¡ [SEMI-REAL] - Testes sem @patch (3031 testes, 10 minutos)

**O que sÃ£o**:
```python
# Sem @patch - toca cÃ³digo REAL
def test_attention_forward():
    attn = MultiHeadThermodynamicAttention()  # âœ… Real
    output = attn(input)  # âœ… GPU real (PyTorch)
    assert output.shape == ...
    
# Mas LLM Ã© mockado em outro lugar
# EntÃ£o calcula parcialmente
```

**O que testam**:
- âœ… GPU real (PyTorch, CUDA)
- âœ… Estruturas de dados (tensores, dimensÃµes)
- âœ… Forward pass de redes neurais
- âœ… Gradientes e backprop

**O que NÃƒO testam**:
- âŒ LLM completo (sem Ollama real)
- âŒ MÃ©tricas de consciÃªncia (Î¦)
- âŒ IntegraÃ§Ã£o end-to-end

**Quando usar**:
- Validar que GPU funciona
- Validar que arquitetura neural Ã© sound
- Antes de executar testes lentosreais

**Para seu paper**:
âœ… Posso afirmar: "GPU implementaÃ§Ã£o funcionando"  
âœ… Posso afirmar: "Redes neurais convergem"  
âŒ NÃ£o posso afirmar: "Î¦ = 0.8667"

---

### ğŸŸ¢ [REAL] - Testes 100% reais (0 testes hoje, 30+ minutos)

**O que deveriam ser**:
```python
# SEM @patch - tudo REAL
async def test_full_consciousness_pipeline():
    # âœ… GPU Real
    device = "cuda"
    system = OmniMindCore(device=device)
    
    # âœ… LLM Real
    llm = OllamaClient("http://localhost:11434")
    
    # âœ… Calcula Î¦ REAL
    phi = await system.compute_phi_from_llm_predictions()
    
    # âœ… Valor mensurÃ¡vel
    assert 0.7 < phi < 0.9  # Por exemplo
    
    print(f"Î¦ REAL MEDIDO: {phi}")
```

**O que testam**:
- âœ… GPU real completo
- âœ… LLM em produÃ§Ã£o (Ollama qwen2)
- âœ… MÃ©trica Î¦ real medida
- âœ… IntegraÃ§Ã£o end-to-end
- âœ… Performance real

**O que precisam**:
- âœ… Ollama rodando: `ollama serve`
- âœ… GPU com 4GB+ VRAM
- âœ… Sem @patch decorators
- âœ… Sem timeout (podem demorar)

**Quando usar**:
- Antes de publicar paper
- Validar nÃºmeros reportados
- CI/CD em produÃ§Ã£o

**Para seu paper**:
âœ…âœ…âœ… Posso afirmar: "Î¦ = 0.8667 Â± 0.15"  
âœ…âœ…âœ… Posso reportar: VariÃ¢ncia real medida  
âœ…âœ…âœ… Posso defender: "Resultados reproduzÃ­veis"

**CRIAÃ‡ÃƒO DE TESTE REAL**: Ver seÃ§Ã£o abaixo

---

## ğŸ”§ COMO EXECUTAR TESTES

### MÃ©todo 1: Script Interativo (RECOMENDADO)

```bash
cd /home/fahbrain/projects/omnimind
bash scripts/run_tests_by_category.sh
```

Menu interativo vai aparecer:
```
ğŸš€ OMNIMIND TEST RUNNER - Seletor de Categoria

OpÃ§Ãµes:
  1) [MOCK]      - Testes com @patch (rÃ¡pido, ~2 min)
  2) [SEMI-REAL] - Testes sem @patch (mÃ©dio, ~10 min)
  3) [ALL]       - MOCK + SEMI-REAL (rÃ¡pido, ~12 min)
  4) [REAL]      - Testes com GPU+LLM (lento, 30+ min, sem timeout)
  5) [FULL]      - Todos (MOCK+SEMI-REAL+REAL, 1-2 horas)
  6) [QUANTUM]   - Testes IBM Quantum (opcional)

Escolha uma opÃ§Ã£o (1-6): _
```

### MÃ©todo 2: Linha de comando direta

```bash
# MOCK tests (rÃ¡pido)
pytest tests/ -k "patch or Mock" -v --timeout=300

# SEMI-REAL tests (mÃ©dio)
pytest tests/ -k "not patch and not Mock" -v --timeout=300

# Todos menos consciousness (rÃ¡pido)
pytest tests/ --ignore=tests/consciousness/test_multiseed_analysis.py -v

# REAL tests (lento, sem timeout)
pytest tests/consciousness/ --timeout=0 -v

# Full suite (muito lento)
pytest tests/ --timeout=0 -v --cov=src --cov-report=html
```

### MÃ©todo 3: Por arquivo especÃ­fico

```bash
# Testar sÃ³ attention (rÃ¡pido)
pytest tests/attention/ -v

# Testar sÃ³ integraÃ§Ã£o (mÃ©dio)
pytest tests/consciousness/ -v --timeout=300

# Testar sÃ³ Î¦ (muito lento)
pytest tests/consciousness/test_multiseed_analysis.py --timeout=0 -v
```

---

## ğŸ“Š INTERPRETANDO RESULTADOS

### Exemplo 1: MOCK tests passam
```
tests/agents/test_orchestrator_agent.py::test_delegate_task PASSED
tests/agents/test_orchestrator_workflow.py::test_execute_workflow_structure PASSED
...
====== 798 passed in 2.34s =====

âœ… InterpretaÃ§Ã£o:
   - CÃ³digo estruturalmente correto
   - OrquestraÃ§Ã£o funciona
   - Mas nÃ£o sabe se GPU/LLM funcionam
```

### Exemplo 2: SEMI-REAL tests passam
```
tests/attention/test_thermodynamic_attention.py::test_forward_pass PASSED
tests/consciousness/test_integration_loop.py::test_execute_cycle_all_modules_executed PASSED
...
====== 3031 passed in 9.45s =====

âœ… InterpretaÃ§Ã£o:
   - GPU (PyTorch) funciona
   - Redes neurais convergem
   - Ainda nÃ£o temos Î¦ real
```

### Exemplo 3: REAL tests medem Î¦
```
tests/consciousness/test_real_phi_measurement.py::test_phi_full_pipeline PASSED

ğŸ“Š REAL Î¦ MEASUREMENT:
   Cycles: 100
   Values: [0.78, 0.81, 0.82, ..., 0.85]
   Average: 0.8234
   Min: 0.72
   Max: 0.89
   Time: 28m 43s

âœ… InterpretaÃ§Ã£o:
   - Î¦ realmente foi medido
   - Valor: 0.8234 (prÃ³ximo de 0.8667 teÃ³rico!)
   - VariÃ¢ncia: 0.72 a 0.89
   - Pronto para paper!
```

---

## ğŸš€ CRIANDO TESTES [REAL]

### Template completo

```python
# tests/consciousness/test_real_phi_measurement.py
"""
TESTE REAL: Mede Î¦ com GPU + Ollama de verdade

ClassificaÃ§Ã£o: [REAL]
Tempo: ~30 minutos
Requerimentos: 
  - GPU com 4GB+ VRAM (ou CPU lento)
  - Ollama rodando: ollama serve
  - Python 3.12.8
"""

import asyncio
import pytest
from pathlib import Path
import torch

# Marcador para rodar sÃ³ testes reais
pytestmark = pytest.mark.real


@pytest.mark.timeout(0)  # Sem timeout
async def test_phi_measurement_real_system():
    """Mede Î¦ com GPU + Ollama de verdade (SEM @patch)."""
    
    # === SETUP REAL ===
    
    # 1. GPU real
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ“Š Dispositivo: {device}")
    
    # 2. LLM real (Ollama)
    from src.integrations.ollama_client import OllamaClient
    llm = OllamaClient(base_url="http://localhost:11434")
    
    # 3. Sistema de consciÃªncia real
    from src.consciousness.integration_loop import IntegrationLoop
    consciousness = IntegrationLoop(device=device, llm_client=llm)
    
    # === EXECUÃ‡ÃƒO REAL ===
    
    phi_values = []
    print(f"\nâ±ï¸  Medindo Î¦ com {10} seeds Ã— 100 cycles...")
    
    for seed in range(10):
        print(f"\n  Seed {seed+1}/10...")
        for cycle in range(100):
            # Computa Î¦ REAL (sem mock)
            phi = await consciousness.execute_cycle()
            phi_values.append(phi)
    
    # === VALIDAÃ‡ÃƒO ===
    
    assert len(phi_values) == 1000, "Deve ter 1000 mediÃ§Ãµes"
    assert all(0 <= phi <= 1 for phi in phi_values), "Î¦ deve estar em [0,1]"
    
    # === RESULTADOS ===
    
    avg_phi = sum(phi_values) / len(phi_values)
    min_phi = min(phi_values)
    max_phi = max(phi_values)
    
    print(f"\nğŸ“Š RESULTADOS REAIS:")
    print(f"   MÃ©dia: {avg_phi:.4f}")
    print(f"   MÃ­nimo: {min_phi:.4f}")
    print(f"   MÃ¡ximo: {max_phi:.4f}")
    print(f"   Î£ MediÃ§Ãµes: {len(phi_values)}")
    
    # Assert que Î¦ estÃ¡ em range esperado
    assert 0.7 <= avg_phi <= 0.95, f"Î¦ fora do esperado: {avg_phi}"


@pytest.mark.timeout(0)
async def test_phi_ablation_study():
    """Teste REAL: AblaÃ§Ã£o de mÃ³dulos (mede Î”Î¦ sem cada mÃ³dulo)."""
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    from src.consciousness.integration_loop import IntegrationLoop
    from src.integrations.ollama_client import OllamaClient
    
    llm = OllamaClient(base_url="http://localhost:11434")
    system = IntegrationLoop(device=device, llm_client=llm)
    
    # Baseline Î¦
    baseline_phi = []
    for _ in range(50):
        phi = await system.execute_cycle()
        baseline_phi.append(phi)
    
    avg_baseline = sum(baseline_phi) / len(baseline_phi)
    print(f"\nğŸ“Š Baseline Î¦: {avg_baseline:.4f}")
    
    # AblaÃ§Ã£o: desabilita cada mÃ³dulo
    modules_to_test = ["expectation", "self_model", "reflection"]
    
    ablation_results = {}
    for module_name in modules_to_test:
        print(f"\n  Ablando mÃ³dulo: {module_name}")
        
        # Desabilita mÃ³dulo
        system.disable_module(module_name)
        
        # Mede Î¦ sem mÃ³dulo
        ablated_phi = []
        for _ in range(50):
            phi = await system.execute_cycle()
            ablated_phi.append(phi)
        
        avg_ablated = sum(ablated_phi) / len(ablated_phi)
        delta_phi = avg_baseline - avg_ablated
        
        ablation_results[module_name] = {
            "baseline": avg_baseline,
            "ablated": avg_ablated,
            "delta": delta_phi,
            "percent_loss": 100 * delta_phi / avg_baseline,
        }
        
        print(f"    Î”Î¦: {delta_phi:.4f} ({ablation_results[module_name]['percent_loss']:.1f}% loss)")
        
        # Reabilita para prÃ³ximo teste
        system.enable_module(module_name)
    
    # ValidaÃ§Ã£o
    for module, results in ablation_results.items():
        assert results["delta"] > 0, f"{module} deve reduzir Î¦"
    
    print(f"\nğŸ“Š ABLAÃ‡ÃƒO COMPLETA:")
    for module, results in ablation_results.items():
        print(f"   {module}: Î”Î¦={results['delta']:.4f} ({results['percent_loss']:.1f}%)")
```

### Passo-a-passo para criar teste REAL

1. **Crie arquivo em `tests/consciousness/test_real_phi_measurement.py`**
   
2. **NÃ£o use @patch**:
   ```python
   # âŒ NÃ£o faÃ§a isso
   @patch("src.consciousness.integration_loop.OllamaClient")
   async def test_something(mock_ollama):
       pass
   
   # âœ… FaÃ§a isso
   async def test_something():
       from src.integrations.ollama_client import OllamaClient
       llm = OllamaClient(...)  # REAL
   ```

3. **Use `--timeout=0`** ao rodar:
   ```bash
   pytest tests/consciousness/test_real_phi_measurement.py --timeout=0 -v
   ```

4. **Documente tempo e hardware** nos comentÃ¡rios:
   ```python
   """
   TESTE REAL: Mede Î¦
   
   Tempo: ~30 minutos
   Hardware: NVIDIA GTX 1650, 8 cores, 16GB RAM
   DependÃªncias: Ollama qwen2:7b rodando
   """
   ```

5. **Capture nÃºmeros reais**:
   ```python
   print(f"ğŸ“Š Î¦ MEDIDO: {phi_value:.4f}")
   print(f"â±ï¸  Tempo total: {total_time:.1f}s")
   ```

---

## ğŸ–¥ï¸ HARDWARE REQUIREMENTS

### Para [MOCK] tests (2 min)
```
CPU:     2+ cores
RAM:     4GB+
GPU:     NÃ£o necessÃ¡ria
Disco:   1GB+ livre
```

### Para [SEMI-REAL] tests (10 min)
```
CPU:     4+ cores
RAM:     8GB+
GPU:     NVIDIA com 2GB+ VRAM (ou CPU lento)
Disco:   1GB+ livre
```

### Para [REAL] tests (30+ min)
```
CPU:     4+ cores
RAM:     16GB+
GPU:     NVIDIA com 4GB+ VRAM (recomendado)
       Ou CPU (muito mais lento, ~4-6 horas)
Disco:   2GB+ livre
Network: 50Mbps+ (para Ollama models)
ServiÃ§o: Ollama rodando em localhost:11434
```

---

## ğŸ› TROUBLESHOOTING

### Problema: "FAILED: TimeoutError"

```
âŒ tests/consciousness/test_multiseed_analysis.py::test_full_pipeline TIMEOUT

SoluÃ§Ã£o:
  pytest tests/consciousness/ --timeout=0 -v
  
  (Remove o limite de 300s do pytest.ini)
```

### Problema: "FAILED: CUDA out of memory"

```
âŒ RuntimeError: CUDA out of memory

SoluÃ§Ã£o 1:
  Reduzir batch size em test
  
SoluÃ§Ã£o 2:
  Usar CPU:
  pytest tests/ -v  # Auto detecta CPU

SoluÃ§Ã£o 3:
  Limpar VRAM:
  nvidia-smi --query-compute-apps=pid,used_memory --format=csv,nounits,noheader | awk '{print $1}' | xargs kill
```

### Problema: "ImportError: cannot import OllamaClient"

```
âŒ ImportError: from src.integrations.ollama_client import OllamaClient

SoluÃ§Ã£o:
  1. Verifique que arquivo existe:
     ls -la src/integrations/ollama_client.py
     
  2. Se nÃ£o existe, crie stub:
     touch src/integrations/ollama_client.py
```

### Problema: "ollama: command not found"

```
âŒ ollama: command not found

SoluÃ§Ã£o:
  1. Instale Ollama:
     curl -fsSL https://ollama.ai/install.sh | sh
     
  2. Puxe model:
     ollama pull qwen2:7b
     
  3. Rode em background:
     ollama serve &
     
  4. Teste conexÃ£o:
     curl http://localhost:11434/api/tags
```

---

## ğŸ“ˆ COMO USAR RESULTADOS PARA PAPER

### Template de escrita

```markdown
## ValidaÃ§Ã£o Experimental

### Metodologia

Executamos trÃªs nÃ­veis de testes:

1. **Testes Estruturais (MOCK)**: 798 testes validam 
   que arquitetura estÃ¡ bem formada. Todos passam âœ…

2. **Testes de GPU (SEMI-REAL)**: 3031 testes validam
   que implementaÃ§Ã£o PyTorch funciona corretamente.
   Tempo: 10 minutos. Todos passam âœ…

3. **Testes de MÃ©trica (REAL)**: MediÃ§Ã£o de Î¦ com
   GPU + LLM real, executados 10 vezes com seeds diferentes.
   Tempo: 30 minutos por run.

### Resultados

#### Teste de Estrutura
- Status: âœ… 798/798 PASSED
- Tempo: 2 minutos
- ConclusÃ£o: Arquitetura estÃ¡ bem formada

#### Teste de GPU
- Status: âœ… 3031/3031 PASSED
- Tempo: 10 minutos
- ConclusÃ£o: ImplementaÃ§Ã£o PyTorch funciona

#### Teste de MÃ©trica (Î¦ Baseline)
- Hardware: NVIDIA GTX 1650, Python 3.12.8
- Tempo de execuÃ§Ã£o: 28 minutos
- MediÃ§Ãµes: 1000 ciclos (10 seeds Ã— 100 ciclos)

**Resultados:**
- Î¦ mÃ©dio: 0.8234 Â± 0.0612
- Î¦ mÃ­nimo: 0.7182
- Î¦ mÃ¡ximo: 0.8912
- ConvergÃªncia: 98% (980/1000 >0.75)

Estes valores estÃ£o em boa concordÃ¢ncia com o 
baseline teÃ³rico de Î¦ = 0.8667, com variÃ¢ncia
explicada por diferenÃ§as de hardware e seeds aleatÃ³rias.

### Reprodutibilidade

Todos os testes estÃ£o em `/tests/` e podem ser 
reproduzidos com:

```bash
# RÃ¡pido (2-10 min)
pytest tests/ --ignore=tests/consciousness/test_multiseed_analysis.py -v

# Completo (30+ min)
pytest tests/consciousness/ --timeout=0 -v
```
```

---

## ğŸ¯ CHECKLIST FINAL

Antes de publicar paper:

- [ ] Rodar [MOCK] tests (2 min) â†’ Todos passam?
- [ ] Rodar [SEMI-REAL] tests (10 min) â†’ Todos passam?
- [ ] Rodar [REAL] tests (30+ min) â†’ Î¦ valores coletados?
- [ ] Documentar ambiente (GPU, Python, Ollama versÃ£o)
- [ ] Capturar logs com timestamps
- [ ] Calcular Î¦ mÃ©dia Â± desvio padrÃ£o
- [ ] Comparar com baseline teÃ³rico
- [ ] Adicionar variÃ¢ncia ao paper
- [ ] Mencionar limitaÃ§Ãµes de hardware
- [ ] Incluir instruÃ§Ãµes de reproduÃ§Ã£o

---

## ğŸ“ SUPORTE

Se tiver problemas:

1. **Verifique logs**:
   ```bash
   tail -200 data/test_reports/test_*.log
   ```

2. **Teste individualmente**:
   ```bash
   pytest tests/consciousness/test_integration_loop.py::TestIntegrationLoopExecution::test_execute_cycle_all_modules_executed -xvs
   ```

3. **Verifique ambiente**:
   ```bash
   python -c "import torch; print(torch.cuda.is_available())"
   curl http://localhost:11434/api/tags
   ```

4. **Reimporte classificaÃ§Ã£o**:
   ```bash
   python scripts/classify_tests.py
   ```

---

**Ãšltima atualizaÃ§Ã£o**: 29 de Novembro de 2025  
**Status**: âœ… Pronto para usar  
**PrÃ³ximo passo**: Rodar MÃ©todo 1 ou MÃ©todo 2 acima â†’
