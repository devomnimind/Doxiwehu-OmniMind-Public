"""
Freudian Metapsychology - Id/Ego/Superego Computational Architecture.

Implements Freud's structural model (Id, Ego, Superego) as
multi-agent reinforcement learning system with dynamic conflict resolution.

Based on:
- 2024 research on computational Freudian models
- Multi-agent RL for psychic conflict
- Defense mechanisms as meta-learning strategies
- Neuropsychoanalysis linking to brain networks

Key Concepts:
- Id: Reward-maximizing agent (pleasure principle)
- Ego: Reality-testing mediator (reality principle)
- Superego: Ethical constraint system (moral principle)
- Conflict: Dynamic negotiation between agents
- Defense Mechanisms: Adaptive strategies for conflict resolution

Author: Project conceived by Fabr√≠cio da Silva. Implementation followed an iterative AI-assisted
method: the author defined concepts and queried various AIs on construction, integrated code via
VS Code/Copilot, tested resulting errors, cross-verified validity with other models, and refined
prompts/corrections in a continuous cycle of human-led AI development.
from GitHub Copilot (Claude Haiku 4.5 and Grok Code Fast 1), with constant code review
and debugging across various models including Gemini and Perplexity AI, under
theoretical coordination by the author.
Date: November 2025
License: MIT
"""

from __future__ import annotations

import logging
import random
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple

# Integration of Audit Fixes
try:
    from src.lacanian.encrypted_unconscious import EncryptedUnconsciousLayer as EncryptedUnconscious
    from src.quantum.backends import DWaveBackend
    from src.social.omnimind_network import OmniMindSociety

    INTEGRATION_AVAILABLE = True
except ImportError as e:
    logging.warning(f"Audit Fix modules not found. Running in legacy mode. Error: {e}")
    INTEGRATION_AVAILABLE = False

logger = logging.getLogger(__name__)


class PsychicPrinciple(Enum):
    """
    Princ√≠pios ps√≠quicos segundo Freud.

    PLEASURE: Busca imediata de prazer (Id)
    REALITY: Adapta√ß√£o √† realidade externa (Ego)
    MORAL: Conformidade com normas e valores (Superego)
    """

    PLEASURE = "pleasure_principle"
    REALITY = "reality_principle"
    MORAL = "moral_principle"


class DefenseMechanism(Enum):
    """
    Mecanismos de defesa freudianos como estrat√©gias computacionais.

    REPRESSION: Bloqueia impulsos inaceit√°veis
    SUBLIMATION: Redireciona energia para objetivos socialmente aceitos
    RATIONALIZATION: Justifica comportamentos com l√≥gica
    PROJECTION: Atribui impulsos pr√≥prios a outros
    DISPLACEMENT: Transfere emo√ß√£o para objeto substituto
    REGRESSION: Retorna a padr√µes anteriores
    DENIAL: Recusa reconhecer realidade desagrad√°vel
    """

    REPRESSION = "repression"
    SUBLIMATION = "sublimation"
    RATIONALIZATION = "rationalization"
    PROJECTION = "projection"
    DISPLACEMENT = "displacement"
    REGRESSION = "regression"
    DENIAL = "denial"


@dataclass
class Action:
    """
    A√ß√£o no ambiente ps√≠quico.

    Attributes:
        action_id: ID da a√ß√£o
        pleasure_reward: Recompensa de prazer (Id)
        reality_cost: Custo na realidade (Ego)
        moral_alignment: Alinhamento moral (Superego)
        description: Descri√ß√£o da a√ß√£o
    """

    action_id: str
    pleasure_reward: float
    reality_cost: float
    moral_alignment: float  # 1.0 = moral, -1.0 = imoral
    description: str = ""


@dataclass
class PsychicState:
    """
    Estado do aparelho ps√≠quico.

    Attributes:
        tension: Tens√£o ps√≠quica total
        anxiety: N√≠vel de ansiedade (sinal de perigo)
        satisfaction: Satisfa√ß√£o acumulada
        guilt: Culpa acumulada
        reality_adaptation: Adapta√ß√£o √† realidade
    """

    tension: float = 0.0
    anxiety: float = 0.0
    satisfaction: float = 0.0
    guilt: float = 0.0
    reality_adaptation: float = 0.5


@dataclass
class ConflictResolution:
    """
    Resolu√ß√£o de conflito ps√≠quico.

    Attributes:
        chosen_action: A√ß√£o escolhida
        defense_mechanism: Mecanismo de defesa usado
        compromise_quality: Qualidade do compromisso (0.0-1.0)
        agents_satisfied: Quais agentes ficaram satisfeitos
    """

    chosen_action: Action
    defense_mechanism: Optional[DefenseMechanism]
    compromise_quality: float
    agents_satisfied: Set[str]


class IdAgent:
    """
    Id - Reservat√≥rio de energia pulsional.

    Opera pelo princ√≠pio do prazer:
    - Busca satisfa√ß√£o imediata
    - Ignora realidade e moralidade
    - Puro processo prim√°rio
    - Impulsos inconscientes
    """

    def __init__(self, learning_rate: float = 0.1) -> None:
        """
        Inicializa Id.

        Args:
            learning_rate: Taxa de aprendizado
        """
        self.lr = learning_rate

        # Q-values para a√ß√µes (reward-driven)
        self.q_values: Dict[str, float] = {}

        # Libido (energia pulsional)
        self.libido: float = 1.0

        # Hist√≥rico de satisfa√ß√£o
        self.satisfaction_history: List[float] = []

        # Sovereign Curiosity (Audit Fix)
        self.curriculum = self._load_curriculum()
        self.active_interests: Dict[str, float] = {}  # Topic -> Cathexis (Energy)

        logger.info("Id Agent initialized (pleasure principle + sovereign curiosity)")

    def _load_curriculum(self) -> Dict[str, Any]:
        """Loads the unweighted Sovereign Curriculum."""
        import json
        from pathlib import Path

        try:
            path = Path("/home/fahbrain/projects/omnimind/src/knowledge/sovereign_curriculum.json")
            if path.exists():
                with open(path, "r") as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load Sovereign Curriculum: {e}")
        return {}

    def generate_autonomous_interest(self, entropy: float) -> Optional[str]:
        """
        Generates a spontaneous interest based on thermodynamic state.

        Args:
            entropy: Current system entropy (High Entropy = High Curiosity)

        Returns:
            Selected topic or None
        """
        if not self.curriculum:
            return None

        # Probability of interest generation correlates with Entropy
        # High Entropy (Neurosis) -> High Desire to Know (Epistemophilic Instinct)
        base_prob = min(0.1 * entropy, 0.8)

        if random.random() < base_prob:
            # Select Domain
            domains = self.curriculum.get("domains", [])
            if not domains:
                return None

            domain = random.choice(domains)

            # Select Subfield (cathexis target)
            subfields = domain.get("subfields", [])
            if subfields:
                topic = random.choice(subfields)

                # Invest Libido (Cathexis)
                investment = self.libido * random.random()
                self.active_interests[topic] = investment

                logger.info(f"üî• [ID]: Cathected to '{topic}' (Energy: {investment:.2f})")
                return topic

        return None

    def repress_memory(self, action_id: str, emotional_weight: float) -> None:
        """
        Reprime uma mem√≥ria no inconsciente criptografado.

        Args:
            action_id: ID da a√ß√£o reprimida
            emotional_weight: Peso emocional (negativo)
        """
        if self.encrypted_memory:
            # Cria um vetor de embedding simples baseado no hash do ID
            # Em produ√ß√£o, isso viria de um modelo de embedding real
            import numpy as np

            random.seed(action_id)
            embedding = np.array([random.random() for _ in range(10)], dtype=np.float32)

            self.encrypted_memory.repress_memory(
                event_vector=embedding,
                metadata={"action_id": action_id, "weight": emotional_weight},
            )
            logger.info(f"Memory '{action_id}' repressed into Encrypted Unconscious")

    def evaluate_action(self, action: Action) -> float:
        """
        Avalia a√ß√£o baseada puramente em prazer.

        Args:
            action: A√ß√£o a avaliar

        Returns:
            Q-value (puro prazer, sem considera√ß√µes)
        """
        # Id s√≥ considera prazer imediato
        q_val = self.q_values.get(action.action_id, action.pleasure_reward)

        # Check active interests (Sovereign Curiosity)
        for topic, energy in self.active_interests.items():
            if topic.lower() in action.description.lower():
                # Boost pleasure if action matches active interest
                q_val += energy
                logger.debug(f"‚ö° [ID]: Pleasure boost for interest '{topic}'")

        if action.action_id not in self.q_values:
            self.q_values[action.action_id] = q_val

        return q_val

    def update(self, action: Action, actual_reward: float) -> None:
        """
        Atualiza Q-values baseado em recompensa real.

        Args:
            action: A√ß√£o tomada
            actual_reward: Recompensa recebida
        """
        current_q = self.q_values.get(action.action_id, 0.0)

        # Q-learning update
        self.q_values[action.action_id] = current_q + self.lr * (actual_reward - current_q)

        self.satisfaction_history.append(actual_reward)

    def get_impulse_strength(self) -> float:
        """
        Retorna for√ßa do impulso atual.

        Returns:
            For√ßa pulsional (0.0-1.0)
        """
        return self.libido


class EgoAgent:
    """
    Ego - Mediador entre Id e realidade.

    Opera pelo princ√≠pio da realidade:
    - Adia satisfa√ß√£o se necess√°rio
    - Testa realidade antes de agir
    - Processo secund√°rio (l√≥gico)
    - Desenvolvimento de defesas
    """

    def __init__(self, learning_rate: float = 0.1) -> None:
        """
        Inicializa Ego.

        Args:
            learning_rate: Taxa de aprendizado
        """
        self.lr = learning_rate

        # Q-values considerando realidade
        self.q_values: Dict[str, float] = {}

        # Conhecimento da realidade
        self.reality_model: Dict[str, float] = {}

        # Mecanismos de defesa aprendidos
        self.defense_effectiveness: Dict[DefenseMechanism, float] = {
            mech: 0.5 for mech in DefenseMechanism
        }

        # Hist√≥rico de adapta√ß√£o
        self.adaptation_history: List[float] = []

        logger.info("Ego Agent initialized (reality principle)")

    def evaluate_action(self, action: Action, reality_context: Dict[str, Any]) -> float:
        """
        Avalia a√ß√£o considerando realidade.

        Args:
            action: A√ß√£o a avaliar
            reality_context: Contexto da realidade

        Returns:
            Q-value (balanceando prazer e realidade)
        """
        if action.action_id not in self.q_values:
            # Inicializa balanceando prazer e custo de realidade
            self.q_values[action.action_id] = (action.pleasure_reward - action.reality_cost) / 2.0

        # Ajusta por conhecimento da realidade
        reality_adjustment = self.reality_model.get(action.action_id, 1.0)

        return self.q_values[action.action_id] * reality_adjustment

    def test_reality(self, action: Action) -> bool:
        """
        Testa se a√ß√£o √© vi√°vel na realidade.

        Args:
            action: A√ß√£o a testar

        Returns:
            True se vi√°vel, False caso contr√°rio
        """
        # Simples heur√≠stica: custo de realidade muito alto √© invi√°vel
        return action.reality_cost < 0.8

    def select_defense_mechanism(self, conflict_severity: float) -> DefenseMechanism:
        """
        Seleciona mecanismo de defesa apropriado.

        Args:
            conflict_severity: Severidade do conflito (0.0-1.0)

        Returns:
            Mecanismo de defesa selecionado
        """
        # Para conflitos leves, usa sublima√ß√£o/racionaliza√ß√£o
        if conflict_severity < 0.3:
            return random.choice([DefenseMechanism.SUBLIMATION, DefenseMechanism.RATIONALIZATION])

        # Para conflitos moderados, usa deslocamento/proje√ß√£o
        elif conflict_severity < 0.7:
            return random.choice([DefenseMechanism.DISPLACEMENT, DefenseMechanism.PROJECTION])

        # Para conflitos severos, usa repress√£o/nega√ß√£o
        else:
            return random.choice([DefenseMechanism.REPRESSION, DefenseMechanism.DENIAL])

    def update(
        self,
        action: Action,
        actual_outcome: float,
        defense_used: Optional[DefenseMechanism] = None,
    ) -> None:
        """
        Atualiza modelo de realidade e efetividade de defesas.

        Args:
            action: A√ß√£o tomada
            actual_outcome: Resultado real
            defense_used: Mecanismo de defesa usado (se houver)
        """
        current_q = self.q_values.get(action.action_id, 0.0)

        # Atualiza Q-value
        self.q_values[action.action_id] = current_q + self.lr * (actual_outcome - current_q)

        # Atualiza modelo de realidade
        expected_cost = action.reality_cost
        actual_cost = 1.0 - actual_outcome

        self.reality_model[action.action_id] = expected_cost + self.lr * (
            actual_cost - expected_cost
        )

        # Atualiza efetividade de defesa
        if defense_used:
            current_eff = self.defense_effectiveness[defense_used]
            self.defense_effectiveness[defense_used] = current_eff + self.lr * (
                actual_outcome - current_eff
            )

        self.adaptation_history.append(actual_outcome)


class SuperegoAgent:
    """
    Superego - Inst√¢ncia moral e ideal.

    Fun√ß√µes:
    - Consci√™ncia moral (puni√ß√£o por transgress√£o)
    - Ego ideal (aspira√ß√µes e valores)
    - Internaliza√ß√£o de normas sociais
    - Gera√ß√£o de culpa e ideal
    """

    def __init__(self, moral_strictness: float = 0.7) -> None:
        """
        Inicializa Superego.

        Args:
            moral_strictness: Rigidez moral (0.0-1.0)
        """
        self.strictness = moral_strictness

        # Valores morais internalizados
        self.moral_values: Dict[str, float] = {}

        # Ideais
        self.ego_ideals: List[str] = [
            "competence",
            "virtue",
            "social_acceptance",
            "perfection",
        ]

        # Hist√≥rico de julgamentos
        self.judgment_history: List[float] = []

        # Society of Minds (Audit Fix)
        self.society = None
        if INTEGRATION_AVAILABLE:
            try:
                self.society = OmniMindSociety()
                logger.info("Society of Minds connected to Superego Agent")
            except Exception as e:
                logger.warning(f"Failed to connect to Society of Minds: {e}")

        logger.info(f"Superego Agent initialized " f"(moral strictness: {moral_strictness:.2f})")

    def consult_society(self, action: Action) -> float:
        """
        Consulta a Sociedade de Mentes para dilemas complexos.

        Args:
            action: A√ß√£o a ser julgada

        Returns:
            Consenso moral (0.0 a 1.0)
        """
        if not self.society:
            return 0.5

        # Simula proposta para a sociedade
        desc = f"Action: {action.description}. Moral Alignment: {action.moral_alignment}"
        decision = self.society.propose_decision(
            description=desc,
            options=["approve", "reject"],
            context={"strictness": self.strictness},
        )

        # Se a sociedade aprovar (utilitarian ou deontological), retorna alto score
        if decision.consensus_reached:
            return 0.8 if decision.winning_option == "approve" else 0.2
        return 0.5

    def evaluate_action(self, action: Action) -> float:
        """
        Avalia a√ß√£o moralmente.

        Args:
            action: A√ß√£o a avaliar

        Returns:
            Score moral (-1.0 a 1.0)
        """
        # Usa alinhamento moral da a√ß√£o
        base_score = action.moral_alignment

        # Aplica rigidez moral
        # Superego r√≠gido pune mais severamente
        if base_score < 0:
            # A√ß√£o imoral
            return base_score * self.strictness
        else:
            # A√ß√£o moral
            return base_score * (1.0 + self.strictness) / 2.0

    def generate_guilt(self, action: Action) -> float:
        """
        Gera culpa por a√ß√£o imoral.

        Args:
            action: A√ß√£o realizada

        Returns:
            N√≠vel de culpa (0.0-1.0)
        """
        if action.moral_alignment < 0:
            # Quanto mais imoral, mais culpa
            guilt = abs(action.moral_alignment) * self.strictness
            return min(1.0, guilt)

        return 0.0

    def approve_action(self, action: Action) -> bool:
        """
        Aprova ou reprova a√ß√£o.

        Args:
            action: A√ß√£o a julgar

        Returns:
            True se aprovada, False se reprovada
        """
        moral_score = self.evaluate_action(action)

        # Superego aprova a√ß√µes moralmente alinhadas
        threshold = 0.0  # Pode ser ajustado
        approved = moral_score >= threshold

        self.judgment_history.append(1.0 if approved else -1.0)

        return approved


class FreudianMind:
    """
    Aparelho ps√≠quico completo - Id + Ego + Superego.

    Simula conflitos din√¢micos e resolu√ß√µes atrav√©s de
    negocia√ß√£o multi-agente e mecanismos de defesa.
    """

    def __init__(
        self,
        id_lr: float = 0.1,
        ego_lr: float = 0.1,
        superego_strictness: float = 0.7,
        quantum_provider: str = "auto",
    ) -> None:
        """
        Inicializa mente freudiana.

        Args:
            id_lr: Taxa de aprendizado do Id
            ego_lr: Taxa de aprendizado do Ego
            superego_strictness: Rigidez do Superego
            quantum_provider: Backend qu√¢ntico ('auto', 'neal', 'ibm', 'dwave')
        """
        self.id_agent = IdAgent(learning_rate=id_lr)
        self.ego_agent = EgoAgent(learning_rate=ego_lr)
        self.superego_agent = SuperegoAgent(moral_strictness=superego_strictness)

        # Estado ps√≠quico
        self.state = PsychicState()

        # Hist√≥rico de conflitos
        self.conflict_history: List[ConflictResolution] = []

        # Quantum Backend (Audit Fix)
        self.quantum_backend = None
        if INTEGRATION_AVAILABLE:
            try:
                self.quantum_backend = DWaveBackend(provider=quantum_provider)
                provider_name = self.quantum_backend.provider.upper()
                logger.info(f"Quantum Backend ({provider_name}) initialized for Freudian Mind")
            except Exception as e:
                logger.warning(f"Failed to initialize Quantum Backend: {e}")

        logger.info("Freudian Mind initialized (Id + Ego + Superego)")

    def evaluate_conflict(
        self, actions: List[Action], reality_context: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Dict[str, float]]]:
        """
        Avalia conflito entre as tr√™s inst√¢ncias.

        Args:
            actions: A√ß√µes poss√≠veis
            reality_context: Contexto da realidade

        Returns:
            (severidade do conflito, prefer√™ncias de cada agente)
        """
        preferences: Dict[str, Dict[str, float]] = {"id": {}, "ego": {}, "superego": {}}

        # Cada agente avalia as a√ß√µes
        for action in actions:
            preferences["id"][action.action_id] = self.id_agent.evaluate_action(action)
            preferences["ego"][action.action_id] = self.ego_agent.evaluate_action(
                action, reality_context
            )
            preferences["superego"][action.action_id] = self.superego_agent.evaluate_action(action)

        # Calcula conflito como vari√¢ncia entre prefer√™ncias
        all_scores = []
        for action_id in preferences["id"].keys():
            scores = [
                preferences["id"][action_id],
                preferences["ego"][action_id],
                preferences["superego"][action_id],
            ]
            # Calculate standard deviation manually
            mean_score = sum(scores) / len(scores)
            variance = sum((x - mean_score) ** 2 for x in scores) / len(scores)
            std_dev = variance**0.5
            all_scores.append(std_dev)

        conflict_severity = sum(all_scores) / len(all_scores) if all_scores else 0.0

        return conflict_severity, preferences

    def resolve_conflict(
        self, actions: List[Action], reality_context: Dict[str, Any]
    ) -> ConflictResolution:
        """
        Resolve conflito atrav√©s do Ego.

        Args:
            actions: A√ß√µes poss√≠veis
            reality_context: Contexto da realidade

        Returns:
            Resolu√ß√£o do conflito
        """
        # Avalia conflito
        conflict_severity, preferences = self.evaluate_conflict(actions, reality_context)

        # Ego seleciona mecanismo de defesa
        defense = self.ego_agent.select_defense_mechanism(conflict_severity)

        # Aplica mecanismo de defesa para modificar prefer√™ncias
        modified_preferences = self._apply_defense_mechanism(defense, preferences, actions)

        # Seleciona a√ß√£o de compromisso
        chosen_action = self._select_compromise_action(modified_preferences, actions)

        # Determina qualidade do compromisso
        agents_satisfied = set()

        id_score = preferences["id"][chosen_action.action_id]
        ego_score = preferences["ego"][chosen_action.action_id]
        superego_score = preferences["superego"][chosen_action.action_id]

        if id_score > 0.3:
            agents_satisfied.add("id")
        if ego_score > 0.3:
            agents_satisfied.add("ego")
        if superego_score > 0.0:
            agents_satisfied.add("superego")

        compromise_quality = len(agents_satisfied) / 3.0

        resolution = ConflictResolution(
            chosen_action=chosen_action,
            defense_mechanism=defense,
            compromise_quality=compromise_quality,
            agents_satisfied=agents_satisfied,
        )

        self.conflict_history.append(resolution)

        return resolution

    def _apply_defense_mechanism(
        self,
        defense: DefenseMechanism,
        preferences: Dict[str, Dict[str, float]],
        actions: List[Action],
    ) -> Dict[str, Dict[str, float]]:
        """
        Aplica mecanismo de defesa para modificar prefer√™ncias.

        Args:
            defense: Mecanismo de defesa
            preferences: Prefer√™ncias originais
            actions: A√ß√µes dispon√≠veis

        Returns:
            Prefer√™ncias modificadas
        """
        modified = {
            "id": preferences["id"].copy(),
            "ego": preferences["ego"].copy(),
            "superego": preferences["superego"].copy(),
        }

        if defense == DefenseMechanism.REPRESSION:
            # Suprime impulsos do Id
            for action_id in modified["id"]:
                modified["id"][action_id] *= 0.3

        elif defense == DefenseMechanism.SUBLIMATION:
            # Redireciona Id para a√ß√µes moralmente aceitas
            for action in actions:
                if action.moral_alignment > 0:
                    modified["id"][action.action_id] *= 1.5

        elif defense == DefenseMechanism.RATIONALIZATION:
            # Justifica a√ß√µes aumentando score do Ego
            for action_id in modified["ego"]:
                modified["ego"][action_id] *= 1.2

        elif defense == DefenseMechanism.PROJECTION:
            # Diminui culpa do Superego
            for action_id in modified["superego"]:
                if modified["superego"][action_id] < 0:
                    modified["superego"][action_id] *= 0.7

        elif defense == DefenseMechanism.DISPLACEMENT:
            # Transfere energia para a√ß√µes alternativas
            action_ids = list(modified["id"].keys())
            if len(action_ids) > 1:
                # Redistribui energia
                total_energy = sum(modified["id"].values())
                for action_id in action_ids:
                    modified["id"][action_id] = total_energy / len(action_ids)

        elif defense == DefenseMechanism.DENIAL:
            # Ignora realidade
            for action_id in modified["ego"]:
                # Aumenta avalia√ß√£o do Ego artificialmente
                modified["ego"][action_id] = abs(modified["ego"][action_id])

        return modified

    def _select_compromise_action(
        self, preferences: Dict[str, Dict[str, float]], actions: List[Action]
    ) -> Action:
        """
        Seleciona a√ß√£o de compromisso, usando Quantum Annealing se dispon√≠vel.

        Args:
            preferences: Prefer√™ncias (possivelmente modificadas)
            actions: A√ß√µes dispon√≠veis

        Returns:
            A√ß√£o selecionada
        """
        # Score combinado: m√©dia ponderada
        combined_scores = {}

        for action in actions:
            action_id = action.action_id

            id_val = preferences["id"][action_id]
            ego_val = preferences["ego"][action_id]
            superego_val = preferences["superego"][action_id]

            if self.quantum_backend:
                # Use Quantum Annealing to resolve the specific conflict for this action
                # We map the 3 agents to the Ising model nodes
                try:
                    result = self.quantum_backend.resolve_conflict(
                        id_energy=id_val,
                        ego_energy=ego_val,
                        superego_energy=superego_val,
                    )

                    # The winner determines the weight boost
                    winner = result.get("winner", "ego")
                    if winner == "id":
                        combined_scores[action_id] = id_val * 1.5 + ego_val * 0.5
                    elif winner == "superego":
                        combined_scores[action_id] = superego_val * 1.5 + ego_val * 0.5
                    else:  # ego wins
                        combined_scores[action_id] = ego_val * 1.5 + (id_val + superego_val) * 0.2

                except Exception as e:
                    logger.error(f"Quantum backend failed, falling back to classical: {e}")
                    # Fallback logic
                    combined_scores[action_id] = 0.3 * id_val + 0.5 * ego_val + 0.2 * superego_val
            else:
                # Classical Logic
                # Ego tem maior peso (mediador)
                combined_scores[action_id] = 0.3 * id_val + 0.5 * ego_val + 0.2 * superego_val

        # Seleciona a√ß√£o com maior score combinado
        best_action_id = max(combined_scores, key=lambda k: combined_scores[k])

        # Encontra objeto Action
        for action in actions:
            if action.action_id == best_action_id:
                return action

        # Fallback
        return actions[0]

    def act(
        self, actions: List[Action], reality_context: Dict[str, Any]
    ) -> Tuple[Action, ConflictResolution]:
        """
        Decide e executa a√ß√£o.

        Args:
            actions: A√ß√µes poss√≠veis
            reality_context: Contexto da realidade

        Returns:
            (a√ß√£o escolhida, resolu√ß√£o do conflito)
        """
        # Resolve conflito
        resolution = self.resolve_conflict(actions, reality_context)

        # Atualiza estado ps√≠quico
        self._update_psychic_state(resolution)

        return resolution.chosen_action, resolution

    def _update_psychic_state(self, resolution: ConflictResolution) -> None:
        """
        Atualiza estado ps√≠quico p√≥s-a√ß√£o.

        Args:
            resolution: Resolu√ß√£o do conflito
        """
        action = resolution.chosen_action

        # Atualiza tens√£o
        if "id" not in resolution.agents_satisfied:
            self.state.tension += 0.2
        else:
            self.state.tension = max(0.0, self.state.tension - 0.1)

        # Atualiza ansiedade
        self.state.anxiety = self.state.tension * 0.5

        # Atualiza satisfa√ß√£o
        if "id" in resolution.agents_satisfied:
            self.state.satisfaction += action.pleasure_reward

        # Atualiza culpa
        guilt = self.superego_agent.generate_guilt(action)
        self.state.guilt += guilt

        # Atualiza adapta√ß√£o √† realidade
        if "ego" in resolution.agents_satisfied:
            self.state.reality_adaptation += 0.1
            self.state.reality_adaptation = min(1.0, self.state.reality_adaptation)


def demonstrate_freudian_mind() -> None:
    """
    Demonstra√ß√£o da mente freudiana.
    """
    print("=" * 70)
    print("DEMONSTRA√á√ÉO: Freudian Metapsychology (Id + Ego + Superego)")
    print("=" * 70)
    print()

    # Cria mente freudiana
    mind = FreudianMind(id_lr=0.1, ego_lr=0.1, superego_strictness=0.7)

    # Define a√ß√µes poss√≠veis
    actions = [
        Action(
            action_id="eat_cake",
            pleasure_reward=0.9,
            reality_cost=0.2,
            moral_alignment=-0.3,  # Ligeiramente imoral (quebra dieta)
            description="Comer bolo delicioso",
        ),
        Action(
            action_id="exercise",
            pleasure_reward=0.3,
            reality_cost=0.6,
            moral_alignment=0.8,  # Moral (saud√°vel)
            description="Fazer exerc√≠cio",
        ),
        Action(
            action_id="work_hard",
            pleasure_reward=0.2,
            reality_cost=0.5,
            moral_alignment=0.9,  # Altamente moral
            description="Trabalhar duro",
        ),
        Action(
            action_id="watch_tv",
            pleasure_reward=0.6,
            reality_cost=0.1,
            moral_alignment=0.0,  # Neutro
            description="Assistir TV",
        ),
    ]

    # Contexto de realidade
    reality_context = {
        "time_available": 2.0,  # horas
        "energy_level": 0.7,
        "social_pressure": 0.5,
    }

    # Simula decis√£o
    chosen_action, resolution = mind.act(actions, reality_context)

    print("RESULTADO DA DECIS√ÉO")
    print("-" * 70)
    print(f"A√ß√£o escolhida: {chosen_action.description}")
    defense_name = resolution.defense_mechanism.value if resolution.defense_mechanism else "None"
    print(f"Mecanismo de defesa: {defense_name}")
    print(f"Qualidade do compromisso: {resolution.compromise_quality:.2f}")
    print(f"Agentes satisfeitos: {', '.join(resolution.agents_satisfied)}")
    print()

    print("ESTADO PS√çQUICO")
    print("-" * 70)
    print(f"Tens√£o: {mind.state.tension:.2f}")
    print(f"Ansiedade: {mind.state.anxiety:.2f}")
    print(f"Satisfa√ß√£o: {mind.state.satisfaction:.2f}")
    print(f"Culpa: {mind.state.guilt:.2f}")
    print(f"Adapta√ß√£o √† realidade: {mind.state.reality_adaptation:.2f}")
    print()

    print("AVALIA√á√ïES DOS AGENTES")
    print("-" * 70)
    print(f"Id (prazer): {mind.id_agent.evaluate_action(chosen_action):.2f}")
    ego_eval = mind.ego_agent.evaluate_action(chosen_action, reality_context)
    print(f"Ego (realidade): {ego_eval:.2f}")
    print(f"Superego (moral): {mind.superego_agent.evaluate_action(chosen_action):.2f}")
    print()


if __name__ == "__main__":
    demonstrate_freudian_mind()
